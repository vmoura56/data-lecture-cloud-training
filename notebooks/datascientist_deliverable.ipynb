{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "\n",
    "from sklearn import set_config; set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a Data Scientist, you don't have access to the full data set, only the 100k on which you've been tasked to train & fine-tune the best model\n",
    "- As an ML Engineer, you'll have access to the full data set later, but not for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"~/.lewagon/mlops/data/raw/train_100k.csv\"\n",
    "df = pd.read_csv(DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1) Compress Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compress our DataFrame by lowering its numeric `dtypes`\n",
    "- from  `float64` to `float32`\n",
    "- from `int64` to `int8`\n",
    "\n",
    "To do so, we iterate on its columns, and for each one, reduce its `dtypes` as much as possible using [`pd.to_numeric`](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html)\n",
    "\n",
    "**💡 1) Read more about `dtype` compression in the ML Ops - Train at Scale lecture on Kitt, \"Appendix A1: Memory Optimization\"**\n",
    "\n",
    "**💡 2) Then, understand and execute the following code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces the size of the DataFrame by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024**2\n",
    "    print(\"old dataframe size: \", round(input_size,2), 'MB')\n",
    "    \n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "\n",
    "    for t in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=t))\n",
    "\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=t)\n",
    "\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "    \n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new DataFrame size: \", round(out_size / 1024**2,2), \" MB\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compress(df, verbose=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check dtypes optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can force an optimal dtype directly at loading time to minimize RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPES_RAW_OPTIMIZED = {\n",
    "    \"key\": \"O\",\n",
    "    \"fare_amount\": \"float32\",\n",
    "    \"pickup_datetime\": \"O\",\n",
    "    \"pickup_longitude\": \"float32\",\n",
    "    \"pickup_latitude\": \"float32\",\n",
    "    \"dropoff_longitude\": \"float32\",\n",
    "    \"dropoff_latitude\": \"float32\",\n",
    "    \"passenger_count\": \"int8\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(DATA_URL, dtype=DTYPES_RAW_OPTIMIZED)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant columns or rows\n",
    "df = df.drop(columns=['key'])\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any', axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove buggy transactions\n",
    "df = df[(df.dropoff_latitude != 0) | (df.dropoff_longitude != 0) | (df.pickup_latitude != 0) | (df.pickup_longitude != 0)]\n",
    "df = df[df.passenger_count > 0]\n",
    "df = df[df.fare_amount > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check NYC bouding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image of NYC map\n",
    "bounding_boxes = (-74.3, -73.7, 40.5, 40.9)\n",
    "\n",
    "url = 'https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/nyc_-74.3_-73.7_40.5_40.9.png'\n",
    "nyc_map = np.array(PIL.Image.open(urllib.request.urlopen(url)))\n",
    "\n",
    "plt.imshow(nyc_map);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant/non-representative transactions (rows) for a training set\n",
    "df = df[df[\"pickup_latitude\"].between(left=40.5, right=40.9)]\n",
    "df = df[df[\"dropoff_latitude\"].between(left=40.5, right=40.9)]\n",
    "df = df[df[\"pickup_longitude\"].between(left=-74.3, right=-73.7)]\n",
    "df = df[df[\"dropoff_longitude\"].between(left=-74.3, right=-73.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cap training set to reasonable values \n",
    "df = df[df.fare_amount < 400]\n",
    "df = df[df.passenger_count < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of fare\n",
    "df.fare_amount.hist(bins=100, figsize=(14,3))\n",
    "plt.xlabel('fare $USD')\n",
    "plt.title('Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used more often to plot data on the NYC map\n",
    "def plot_on_map(df, BB, nyc_map, s=10, alpha=0.2):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16,10))\n",
    "\n",
    "    axs[0].scatter(df.pickup_longitude, df.pickup_latitude, zorder=1, alpha=alpha, c='red', s=s)\n",
    "    axs[0].set_xlim((BB[0], BB[1]))\n",
    "    axs[0].set_ylim((BB[2], BB[3]))\n",
    "    axs[0].set_title('Pickup locations')\n",
    "    axs[0].imshow(nyc_map, zorder=0, extent=BB)\n",
    "\n",
    "    axs[1].scatter(df.dropoff_longitude, df.dropoff_latitude, zorder=1, alpha=alpha, c='blue', s=s)\n",
    "    axs[1].set_xlim((BB[0], BB[1]))\n",
    "    axs[1].set_ylim((BB[2], BB[3]))\n",
    "    axs[1].set_title('Dropoff locations')\n",
    "    axs[1].imshow(nyc_map, zorder=0, extent=BB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training data on map\n",
    "plot_on_map(df, bounding_boxes, nyc_map, s=1, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_on_map(df, bounding_boxes, nyc_map, s=20, alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hires(df, BB, figsize=(12, 12), ax=None, c=('r', 'b')):\n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    \n",
    "    def select_within_boundingbox(df, BB):\n",
    "        return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n",
    "            (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n",
    "            (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n",
    "            (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n",
    "            \n",
    "    idx = select_within_boundingbox(df, BB)\n",
    "    ax.scatter(df[idx].pickup_longitude, df[idx].pickup_latitude, c=\"red\", s=0.01, alpha=0.5)\n",
    "    ax.scatter(df[idx].dropoff_longitude, df[idx].dropoff_latitude, c=\"blue\", s=0.01, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hires(df, (-74.1, -73.7, 40.6, 40.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hires(df, (-74, -73.95, 40.7, 40.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Baseline Score  - Preliminary Intuitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline model should at least take into account the most obvious feature: the distance between `pickup` and `dropoff`.\n",
    "\n",
    "The correct distance metric is appropriately named \"Manhattan distance\" (L1 distance), which computes the sum of horizontal and vertical distances between two points, instead of the diagonal (Euclidean, L2) distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def manhattan_distance(start_lat: float, start_lon: float, end_lat: float, end_lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance between in km two points on the earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "    \n",
    "    lat_1_rad, lon_1_rad = math.radians(start_lat), math.radians(start_lon)\n",
    "    lat_2_rad, lon_2_rad = math.radians(end_lat), math.radians(end_lon)\n",
    "    \n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "    \n",
    "    manhattan_rad = abs(dlon_rad) + abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "    \n",
    "    return manhattan_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.apply(lambda row: manhattan_distance(row[\"pickup_latitude\"], row[\"pickup_longitude\"], row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ The following code takes a while and is not optimized.\n",
    "\n",
    "Applying a mapping function row-by-row on a DataFrame is a very bad engineering practice, as DataFrames are stored in memory \"column-by-column\". We talk about \"column-major\" data storage formats. \n",
    "\n",
    "`df.apply(..., axis=1)` is equivalent to a python `for` loop, and does not harness NumPy's vectorized operations.\n",
    "\n",
    "👇 Let's vectorize our code instead! Notice the improvement by a factor of several hundred 💪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the Manhattan distance in km between two points on the earth (specified in decimal degrees).\n",
    "    Vectorized version for pandas df\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "    \n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "    \n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "    \n",
    "    manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "    \n",
    "    return manhattan_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "manhattan_distance_vectorized(df, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distance'] = manhattan_distance_vectorized(df, \"pickup_latitude\", \"pickup_longitude\",\"dropoff_latitude\", \"dropoff_longitude\")\n",
    "df['distance'].hist(bins=50)\n",
    "plt.title(\"distance (km)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df, x='distance', y='fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearson, p_value = pearsonr(df['distance'], df['fare_amount'])\n",
    "print(f'{pearson=}')\n",
    "print(f'{p_value=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "mae = -1*cross_val_score(LinearRegression(), X=df[['distance']], y=df['fare_amount'], scoring='neg_mean_absolute_error').mean()\n",
    "print(f'baseline mae = {round(mae,2)} $')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ We've got our baseline! \n",
    "\n",
    "Let's drop the `distance` column we added manually, and create a true preprocessing pipeline now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['distance'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a dataset with only 5 features (passengers + lon/lat), and potentially dozens of millions of rows.\n",
    "\n",
    "👉 It makes perfect sense to create a lot of \"engineered\" features such as \"hour of the day\"  \n",
    "- Hundreds of them would cause no problem because the huge number of rows will allow our model to learn all weights associated with these multiple features\n",
    "- A dense, Deep Learning network will be well-suited for such a case\n",
    "\n",
    "❗️ The proposed preprocessor below outputs a **fixed number of features** (65) that is **independent of the training set**. You will see that it will come in handy when scaling it up to hundreds of millions of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"fare_amount\", axis=1)\n",
    "y = df[[\"fare_amount\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Passenger Preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze passenger numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.passenger_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSENGER PIPE\n",
    "p_min = 1\n",
    "p_max = 8\n",
    "passenger_pipe = FunctionTransformer(lambda p: (p-p_min)/(p_max-p_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "    ],\n",
    ")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessor.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Time Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's extract category attributes from the `pickup_datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def transform_time_features(X: pd.DataFrame) -> np.ndarray:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    pickup_dt = pd.to_datetime(\n",
    "        X[\"pickup_datetime\"],\n",
    "        format=\"%Y-%m-%d %H:%M:%S UTC\",\n",
    "        utc=True\n",
    "    )\n",
    "\n",
    "    pickup_dt = pickup_dt.dt.tz_convert(\"America/New_York\").dt\n",
    "\n",
    "    dow = pickup_dt.weekday\n",
    "    hour = pickup_dt.hour\n",
    "    month = pickup_dt.month\n",
    "    year = pickup_dt.year\n",
    "\n",
    "    hour_sin = np.sin(2 * math.pi / 24 * hour)\n",
    "    hour_cos = np.cos(2*math.pi / 24 * hour)\n",
    "    \n",
    "    return np.stack([hour_sin, hour_cos, dow, month, year], axis=1)\n",
    "\n",
    "X_time_processed = transform_time_features(X[[\"pickup_datetime\"]])\n",
    "\n",
    "pd.DataFrame(X_time_processed, columns=[\"hour_sin\", \"hour_cos\", \"dow\", \"month\", \"year\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, one-hot-encode [\"day of week\", \"month\"] by forcing all 24*7 combinations of categories to be always present in X_processed (remember we want a fixed size for X_processed at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_categories = {\n",
    "        0: np.arange(0, 7, 1),  # days of the week from 0 to 6\n",
    "        1: np.arange(1, 13, 1)  # months of the year from 1 to 12\n",
    "    }\n",
    "\n",
    "OneHotEncoder(categories=time_categories, sparse=False)\\\n",
    "    .fit_transform(X_time_processed[:,[2,3]]) # column index [2,3] for ['dow', 'month'] !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And combine this with a re-scaling of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.pickup_datetime.min())\n",
    "print(df.pickup_datetime.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_min = 2009\n",
    "year_max = 2019 # Our model may extend in the future. No big deal if the scaled data extend slightly beyond 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_time_features),\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(\n",
    "            categories=time_categories,\n",
    "            sparse=False,\n",
    "            handle_unknown=\"ignore\"\n",
    "        ), [2, 3]), # corresponds to columns [\"day of week\", \"month\"], not the other columns\n",
    "\n",
    "        (FunctionTransformer(lambda year: (year - year_min) / (year_max - year_min)), [4]), # min-max scale the columns 4 [\"year\"]\n",
    "        remainder=\"passthrough\" # keep hour_sin and hour_cos\n",
    "    )\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessor.fit_transform(X_train)).plot(kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ 23 features approximately centered and scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Distance Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add both the haversine and Manhattan distances as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat_features = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the haversine and Manhattan distances between two points (specified in decimal degrees).\n",
    "    Vectorized version for pandas df\n",
    "    Computes distance in Km\n",
    "    \"\"\"\n",
    "    earth_radius = 6371\n",
    "\n",
    "    lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "    lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "\n",
    "    dlon_rad = lon_2_rad - lon_1_rad\n",
    "    dlat_rad = lat_2_rad - lat_1_rad\n",
    "\n",
    "    manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "    manhattan_km = manhattan_rad * earth_radius\n",
    "\n",
    "    a = (np.sin(dlat_rad / 2.0)**2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon_rad / 2.0)**2)\n",
    "    haversine_rad = 2 * np.arcsin(np.sqrt(a))\n",
    "    haversine_km = haversine_rad * earth_radius\n",
    "\n",
    "    return dict(\n",
    "        haversine = haversine_km,\n",
    "        manhattan = manhattan_km\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lonlat_features(X:pd.DataFrame)-> pd.DataFrame:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    res = distances_vectorized(X, *lonlat_features)\n",
    "\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "distances = transform_lonlat_features(X[lonlat_features])\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_min = 0\n",
    "dist_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_lonlat_features),\n",
    "    FunctionTransformer(lambda dist: (dist - dist_min) / (dist_max - dist_min))\n",
    "    )\n",
    "distance_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "        (\"dist_preproc\", distance_pipe, lonlat_features),\n",
    "    ],\n",
    ")\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = pd.DataFrame(preprocessor.fit_transform(X_train))\n",
    "X_processed.plot(kind='box');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ 25 features, approximately scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) GeoHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add information about **districts**! \n",
    "\n",
    "Some might be more expensive than others to go to/depart from (e.g. JFK airport!)\n",
    "\n",
    "To _bucketize_ geospatial information, we'll use `pygeohash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash as gh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 pygeohash converts (lat,lon) into geospacial \"squared buckets\" of chosen precisions. The more precision you ask, the more \"buckets\" possibility there is!\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/geohashes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = X_train.iloc[0,:]\n",
    "(x0.pickup_latitude, x0.pickup_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=3))\n",
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=4))\n",
    "print(gh.encode(x0.pickup_latitude, x0.pickup_longitude, precision=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Let's apply it to ALL of our data set (note that this preprocessing may take a very long time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_geohash(X:pd.DataFrame, precision:int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add a geohash (ex: \"dr5rx\") of len \"precision\" = 5 by default\n",
    "    corresponding to each (lon, lat) tuple, for pick-up and drop-off\n",
    "    \"\"\"\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    X[\"geohash_pickup\"] = X.apply(\n",
    "        lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "    X[\"geohash_dropoff\"] = X.apply(\n",
    "        lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return X[[\"geohash_pickup\", \"geohash_dropoff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_geohash(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ Notice that this time, we have no choice but to apply `pygeohash` row by row with `df.apply(axis=1)`, and it takes a while to compute.\n",
    "\n",
    "This is the danger of relying on an external Python library that is not always vectorized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 What are the most common districts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_geohashes = pd.concat([\n",
    "    X_train.apply(lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=5), axis=1),\n",
    "    X_train.apply(lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=5), axis=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_geohashes.value_counts()))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(np.cumsum(all_geohashes.value_counts()[:20])/(2*len(X_train))*100)\n",
    "plt.title(\"percentage of taxi rides from/to these districts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ Only the first 20 districts matter. We can one-hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_geohash_districts = np.array(all_geohashes.value_counts()[:20].index)\n",
    "most_important_geohash_districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's hard-code below the 20 most frequent district GeoHashes of precision 5,\n",
    "# covering about 99% of all dropoff/pickup locations.\n",
    "most_important_geohash_districts = [\n",
    "    \"dr5ru\", \"dr5rs\", \"dr5rv\", \"dr72h\", \"dr72j\", \"dr5re\", \"dr5rk\",\n",
    "    \"dr5rz\", \"dr5ry\", \"dr5rt\", \"dr5rg\", \"dr5x1\", \"dr5x0\", \"dr72m\",\n",
    "    \"dr5rm\", \"dr5rx\", \"dr5x2\", \"dr5rw\", \"dr5rh\", \"dr5x8\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's one-hot encode each GeoHash in one of the top-20 different buckets listed above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geohash_categories = {\n",
    "    0: most_important_geohash_districts,  # pickup district list\n",
    "    1: most_important_geohash_districts  # dropoff district list\n",
    "}\n",
    "\n",
    "geohash_pipe = make_pipeline(\n",
    "    FunctionTransformer(compute_geohash),\n",
    "    OneHotEncoder(\n",
    "        categories=geohash_categories,\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse=False\n",
    "    )\n",
    ")\n",
    "geohash_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5) Full Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap our final preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygeohash as gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_time_features(X: pd.DataFrame)->np.ndarray:\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    \n",
    "    pickup_dt = pd.to_datetime(\n",
    "        X[\"pickup_datetime\"],\n",
    "        format=\"%Y-%m-%d %H:%M:%S UTC\",\n",
    "        utc=True\n",
    "    )\n",
    "\n",
    "    pickup_dt = pickup_dt.dt.tz_convert(\"America/New_York\").dt\n",
    "\n",
    "    dow = pickup_dt.weekday\n",
    "    hour = pickup_dt.hour\n",
    "    month = pickup_dt.month\n",
    "    year = pickup_dt.year\n",
    "\n",
    "    hour_sin = np.sin(2 * math.pi / 24 * hour)\n",
    "    hour_cos = np.cos(2*math.pi / 24 * hour)\n",
    "    \n",
    "    result = np.stack([hour_sin, hour_cos, dow, month, year], axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lonlat_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "    lonlat_features = [\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"]\n",
    "\n",
    "    def distances_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the haversine and Manhattan distances between two points on the earth (specified in decimal degrees).\n",
    "        Vectorized version for pandas df\n",
    "        Computes distance in Km\n",
    "        \"\"\"\n",
    "        earth_radius = 6371\n",
    "\n",
    "        lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])\n",
    "        lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])\n",
    "\n",
    "        dlon_rad = lon_2_rad - lon_1_rad\n",
    "        dlat_rad = lat_2_rad - lat_1_rad\n",
    "\n",
    "        manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)\n",
    "        manhattan_km = manhattan_rad * earth_radius\n",
    "\n",
    "        a = (np.sin(dlat_rad / 2.0)**2 + np.cos(lat_1_rad) * np.cos(lat_2_rad) * np.sin(dlon_rad / 2.0)**2)\n",
    "        haversine_rad = 2 * np.arcsin(np.sqrt(a))\n",
    "        haversine_km = haversine_rad * earth_radius\n",
    "\n",
    "        return dict(\n",
    "            haversine=haversine_km,\n",
    "            manhattan=manhattan_km)\n",
    "\n",
    "    result = pd.DataFrame(distances_vectorized(X, *lonlat_features))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_geohash(X: pd.DataFrame, precision: int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add a GeoHash (ex: \"dr5rx\") of len \"precision\" = 5 by default\n",
    "    corresponding to each (lon, lat) tuple, for pick-up, and drop-off\n",
    "    \"\"\"\n",
    "    assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "    X[\"geohash_pickup\"] = X.apply(\n",
    "        lambda x: gh.encode(x.pickup_latitude, x.pickup_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "    X[\"geohash_dropoff\"] = X.apply(\n",
    "        lambda x: gh.encode(x.dropoff_latitude, x.dropoff_longitude, precision=precision),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return X[[\"geohash_pickup\", \"geohash_dropoff\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSENGER PIPE\n",
    "p_min = 1\n",
    "p_max = 8\n",
    "passenger_pipe = FunctionTransformer(lambda p: (p - p_min) / (p_max - p_min))\n",
    "\n",
    "# DISTANCE PIPE\n",
    "dist_min = 0\n",
    "dist_max = 100\n",
    "\n",
    "distance_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_lonlat_features),\n",
    "    FunctionTransformer(lambda dist: (dist - dist_min) / (dist_max - dist_min))\n",
    ")\n",
    "\n",
    "# TIME PIPE\n",
    "year_min = 2009\n",
    "year_max = 2019\n",
    "\n",
    "time_categories = {\n",
    "    0: np.arange(0, 7, 1),  # days of the week\n",
    "    1: np.arange(1, 13, 1)  # months of the year\n",
    "}\n",
    "\n",
    "time_pipe = make_pipeline(\n",
    "    FunctionTransformer(transform_time_features),\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(\n",
    "            categories=time_categories,\n",
    "            sparse=False,\n",
    "            handle_unknown=\"ignore\"\n",
    "        ), [2,3]), # corresponds to columns [\"day of week\", \"month\"], not the other columns\n",
    "\n",
    "        (FunctionTransformer(lambda year: (year - year_min) / (year_max - year_min)), [4]), # min-max scale the columns 4 [\"year\"]\n",
    "        remainder=\"passthrough\" # keep hour_sin and hour_cos\n",
    "    )\n",
    ")\n",
    "\n",
    "# GEOHASH PIPE\n",
    "lonlat_features = [\n",
    "    \"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\",\n",
    "    \"dropoff_longitude\"\n",
    "]\n",
    "\n",
    "# Below are the 20 most frequent district geohashes of precision 5,\n",
    "# covering about 99% of all dropoff/pickup locations,\n",
    "# according to prior analysis in a separate notebook\n",
    "most_important_geohash_districts = [\n",
    "    \"dr5ru\", \"dr5rs\", \"dr5rv\", \"dr72h\", \"dr72j\", \"dr5re\", \"dr5rk\",\n",
    "    \"dr5rz\", \"dr5ry\", \"dr5rt\", \"dr5rg\", \"dr5x1\", \"dr5x0\", \"dr72m\",\n",
    "    \"dr5rm\", \"dr5rx\", \"dr5x2\", \"dr5rw\", \"dr5rh\", \"dr5x8\"\n",
    "]\n",
    "\n",
    "geohash_categories = {\n",
    "    0: most_important_geohash_districts,  # pickup district list\n",
    "    1: most_important_geohash_districts  # dropoff district list\n",
    "}\n",
    "\n",
    "geohash_pipe = make_pipeline(\n",
    "    FunctionTransformer(compute_geohash),\n",
    "    OneHotEncoder(\n",
    "        categories=geohash_categories,\n",
    "        handle_unknown=\"ignore\",\n",
    "        sparse=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# COMBINED PREPROCESSOR\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"passenger_scaler\", passenger_pipe, [\"passenger_count\"]),\n",
    "        (\"time_preproc\", time_pipe, [\"pickup_datetime\"]),\n",
    "        (\"dist_preproc\", distance_pipe, lonlat_features),\n",
    "        (\"geohash\", geohash_pipe, lonlat_features),\n",
    "    ],\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = final_preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "pd.DataFrame(X_train_processed).plot(kind='box', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "sns.heatmap(pd.DataFrame(X_train_processed).corr(), vmin=-1, cmap='RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, we compress our data to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_processed.nbytes / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the data a bit\n",
    "X_train_processed = X_train_processed.astype(np.float32)\n",
    "print(X_train_processed.nbytes / 1024**2, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ The preprocessor outputs a **fixed** number of features (65) that is independent of the training set. \n",
    "\n",
    "☝️ The preprocessor is also  **state-less** (i.e it has no `.fit()` method, only a `.transform()`). It can be seen as a *pure function* $f:X \\rightarrow X_{processed}$ without an internal state, as opposed to standard scaling for instance, which has to store \"X_train standard deviations\" as internal states.\n",
    "\n",
    "These two features will make work much easier for the ML Engineering team to scale preprocessing to hundreds of GBs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ To begin with, please install `tensorflow` >=2.8 on your `taxifare-env`\n",
    "\n",
    "For instance, you can add one of the following lines to `requirements.txt`\n",
    "\n",
    "```\n",
    "tensorflow-macos==2.10.0 ; sys_platform == \"darwin\"\n",
    "tensorflow==2.10.0 ; sys_platform == \"linux\"\n",
    "```\n",
    "\n",
    "And then `pip install -e .` to install it on `taxifare-env`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential, layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(X: np.ndarray) -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights\n",
    "    \"\"\"\n",
    "\n",
    "    reg = regularizers.l1_l2(l2=0.01)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.BatchNormalization(input_shape=X.shape[1:]))\n",
    "    model.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=reg, input_shape=X.shape[1:]))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Dense(50, activation=\"relu\", kernel_regularizer=reg))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Dense(10, activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.99)) # use momentum=0 for to only use statistic of the last seen minibatch in inference mode (\"short memory\"). Use 1 to average statistics of all seen batch during training histories.\n",
    "    \n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "    print(\"✅ model initialized\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X_train_processed)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed = final_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.hist(y_pred, label='pred', color='r', bins=200, alpha=0.3)\n",
    "plt.hist(y_test, label='truth', color='b', bins=200, alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim((0,60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_pred - y_test\n",
    "sns.histplot(residuals)\n",
    "plt.xlim(xmin=-20, xmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.sort_values(by='fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual vs. actual scatter plot\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(x=y_test,y=residuals, alpha=0.1)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual vs. predicted scatter plot\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(x=y_pred,y=residuals, alpha=0.1)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️ Our model has an MAE of about 2$ per course, compared with a mean course price of 11$.  \n",
    "\n",
    "A simple linear regression would give us about 2.5$ of MAE, but the devil lies in the details!\n",
    "\n",
    "In particular, we're not that good at predicting very long/expensive courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧪 Test Your Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Try to answer these questions with your buddy\n",
    "1. Are you satisfied with the model's performance?\n",
    "2. What is a stateless pipeline (as opposed to a stateful one)?\n",
    "3. How does an OHEncoder work with fixed column categories?\n",
    "4. How is the data normalization done in the Neural Net?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary markdown='span'>💡 Answers</summary>\n",
    "\n",
    "1. We have a 25% improvement over the linear regression baseline, and it seems that our model has been doing its best, given the features it has been provided with. Besides, a 2$ forecast error on taxi courses whose prices also depend on traffic seems close to the irreducible error rate.\n",
    "2. A stateless pipeline has no real `.fit()` method, only a `.transform()`. \n",
    "3. To become stateless, we've hard-coded the `categories` to one-hot encode `OneHotEncoder(categories=categories,...)` and hard-coded the statistical features of each column in our scalers:  `FunctionTransformer(lambda dist: (dist - dist_min)/(dist_max - dist_min))`\n",
    "4. In the TensorFlow model, notice the `layers.BatchNormalization()` we've added between each dense layer, which normalizes data batch-per-batch! It's a cool feature to help fix vanish gradients during the back-propagation!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Predict the price for this new course `X_new` below and store the result `y_new` as a `np.ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame(dict(\n",
    "    key=[\"2013-07-06 17:18:00\"],  # useless but the pipeline requires it\n",
    "    pickup_datetime=[\"2013-07-06 17:18:00 UTC\"],\n",
    "    pickup_longitude=[-73.950655],\n",
    "    pickup_latitude=[40.783282],\n",
    "    dropoff_longitude=[-73.984365],\n",
    "    dropoff_latitude=[40.769802],\n",
    "    passenger_count=[1]\n",
    "))\n",
    "\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'notebook',\n",
    "    subdir='train_at_scale',\n",
    "    y_new=y_new\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
